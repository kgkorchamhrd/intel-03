# -*- coding: utf-8 -*-
"""HW4_GD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11aPMJCt010QCTbvsef8hI2SRUEKkn6L_
"""

#vector Gradient 그려보기

import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-2,2,11)
y = np.linspace(-2,2,11)

print(x)
print(y)

x,y = np.meshgrid(x,y)
print(x)
print(y)

f = lambda x,y: (x-1)**2 + (y-1)**2
z = f(x,y)
print(z)

grad_f_x = lambda x,y: 2*(x-1)
grad_f_y = lambda x,y: 2*(y-1)

dz_dx = grad_f_x(x,y)
dz_dy = grad_f_y(x,y)

print(dz_dx)
print(dz_dy)

ax = plt.axes()
ax.contour(x,y,z,level=np.linspace(0,10,20),cmap=plt.cm.jet)
ax.quiver(x,y, -dz_dx,-dz_dy)
ax.grid()
ax.axis('equal')
ax.set_xlabel('$x$')
ax.set_ylabel('$y$')
plt.show()

#𝒇 𝒙 = 𝒙𝟐 − 𝟒𝒙 + 𝟔 함수를 아래 조건을 이용하는
#Gradient descent방법을 이용하여 최소값을 찾으시오


def f(x):
  return x**2-4*x+6

NumberOfPoints =101
x = np.linspace(-5,5,NumberOfPoints)
fx = f(x)
print(x)
print(fx)
plt.plot(x,fx)
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')


plt.show()

xid = np.argmin(fx)
xopt = x[xid]
print(xopt,f(xopt))

plt.plot(x,fx)
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')

plt.plot(xopt,f(xopt),'xr')
plt.show()

# 초기값 0, learning rate을 1.2로 한 경우

def grad_fx(x):
  return 2*x-4

def steepest_descent(func, grad_func, x0, learning_rate=0.1, max_iter=10,verbose=True):
    paths = []
    for i in range(max_iter):
        x1 =x0 - learning_rate*grad_func(x0)
        if verbose:
          print('{0:03d} : {1:4.3f}, {2:4.2E}'.format(i,x1,func(x1)))
        x0 = x1
        paths.append(x0)


    return (x0,func(x0),paths)


xopt,fopt,paths = steepest_descent(f,grad_fx,0.0,learning_rate=1.2)

x = np.linspace(0.5,2.5,1000)
paths = np.array(paths)
plt.plot(x,f(x))
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')
plt.plot(paths,f(paths),'o-')
plt.show()


plt.plot(f(paths), 'o-')
plt.grid()
plt.xlabel('x')
plt.ylabel('cost')
plt.title('plot of cost')
plt.show()

# 초기값 1, learning rate을 1로 한 경우

xopt, fopt, paths = steepest_descent(f,grad_fx,1.0,learning_rate=1.0)

x = np.linspace(0.5,3.5,1000)
paths = np.array(paths)
plt.plot(x,f(x))
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')
plt.plot(paths,f(paths),'o-')
plt.show()

plt.plot(f(paths), 'o-')
plt.grid()
plt.xlabel('x')
plt.ylabel('cost')
plt.title('plot of cost')
plt.show()

# 초기값 1, learning rate을 0.001로 한 경우

xopt, fopt, paths = steepest_descent(f,grad_fx,1.0,learning_rate=0.001)

x = np.linspace(0.5,3.5,1000)
paths = np.array(paths)
plt.plot(x,f(x))
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')
plt.plot(paths,f(paths),'o-')
plt.show()

plt.plot(f(paths))
plt.grid()
plt.xlabel('x')
plt.ylabel('cost')
plt.title('plot of cost')
plt.show()

# 초기값 3, learning rate을 0.9로 한 경우

xopt, fopt, paths = steepest_descent(f,grad_fx,3.0,learning_rate=0.9)

x = np.linspace(0.5,3.5,1000)
paths = np.array(paths)
plt.plot(x,f(x))
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('plot of f(x)')
plt.plot(paths,f(paths),'o-')
plt.show()

plt.plot(f(paths))
plt.grid()
plt.xlabel('x')
plt.ylabel('cost')
plt.title('plot of cost')
plt.show()

# Gradient descent실습:2차원 확장

import numpy as np
import matplotlib.pylab as plt
from matplotlib.colors import LogNorm
from mpl_toolkits.mplot3d import Axes3D

def contour(f, x, y, level = np.logspace(0, 5, 35)):
	fig, ax = plt.subplots(figsize=(8, 8))

	ax.contour(x, y, f(x,y), levels=level, norm=LogNorm(), cmap=plt.cm.jet)

	ax.set_xlabel('$x$')
	ax.set_ylabel('$y$')

	# ax.set_xlim((xmin, xmax))
	# ax.set_ylim((ymin, ymax))

	plt.show()

def contour_with_quiver(f, x, y, grad_x, grad_y, norm=LogNorm(), level = np.logspace(0, 5, 35),
	minima=None):
	dz_dx = grad_x(x,y)
	dz_dy = grad_y(x,y)

	fig, ax = plt.subplots(figsize=(6, 6))

	ax.contour(x, y, f(x,y), levels=level, norm=norm, cmap=plt.cm.jet)
	if minima is not None:
		ax.plot(*minima, 'r*', markersize=18)
	ax.quiver(x, y, -dz_dx, -dz_dy, alpha=.5)
	ax.set_xlabel('$x$')
	ax.set_ylabel('$y$')

	# ax.set_xlim((xmin, xmax))
	# ax.set_ylim((ymin, ymax))

	plt.show()

def surf(f, x, y, norm=LogNorm(), minima=None):
	fig = plt.figure(figsize=(8, 5))
	ax = plt.axes(projection='3d', elev=50, azim=-50)

	ax.plot_surface(x, y, f(x,y), norm=norm, rstride=1, cstride=1,
	                edgecolor='none', alpha=.8, cmap=plt.cm.jet)

	if minima is not None:
		ax.plot(*minima, f(*minima), 'r*', markersize=10)

	ax.set_xlabel('$x$')
	ax.set_ylabel('$y$')
	ax.set_zlabel('$z$')

	plt.show()

def contour_with_path(f, x, y, paths, norm=LogNorm(), level=np.logspace(0, 5, 35), minima=None):
	fig, ax = plt.subplots(figsize=(6, 6))

	ax.contour(x, y, f(x,y), levels=level, norm=norm, cmap=plt.cm.jet)
	ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')
	if minima is not None:
		ax.plot(*minima, 'r*', markersize=18)

	ax.set_xlabel('$x$')
	ax.set_ylabel('$y$')

	# ax.set_xlim((xmin, xmax))
	# ax.set_ylim((ymin, ymax))

	plt.show()













#𝑓(𝑥,𝑦)=(𝑥−2)2+(𝑦−2)2함수를 Gradient descent방법을 이용하여 최소값을 찾으시오


xmin, xmax, xstep = -4.0, 4.0, .25
ymin, ymax, ystep = -4.0, 4.0, .25

x,y = np.meshgrid(np.arange(xmin, xmax+xstep, xstep),
                  np.arange(ymin, ymax+ystep, ystep))

f = lambda x,y: (x-2)**2+(y-2)**2
z = f(x,y)
minima = np.array([2,2])
f(*minima)

minima_ = minima.reshape(-1,1)
print(minima,minima_)
surf(f,x,y,minima=minima_)

grad_f_x = lambda x,y: 2*(x-2)
grad_f_y = lambda x,y: 2*(y-2)

contour_with_quiver(f,x,y,grad_f_x,grad_f_y,minima=minima_)


def steepest_descent_twod(func, gradx, grady, x0, Maxlter=10, learning_rate =0.25,verbose=True):

    paths = [x0]
    fval_paths = [f(x0[0],x0[1])]
    for i in range(Maxlter):
        x1 = x0 - learning_rate*np.array([gradx(x0[0],x0[1]),grady(x0[0],x0[1])])
        fval = f(*x1)
        if verbose:
          print(i,x1,fval)
        x0 = x1
        paths.append(x0)
        fval_paths.append(fval)
    paths = np.array(paths)
    paths = np.array(np.matrix(paths).T)
    fval_paths = np.array(fval_paths)
    return (x0,fval,paths,fval_paths)


x0=np.array([-2.,-2.])
xopt,fopt,paths,fval_paths = steepest_descent_twod(f,grad_f_x,grad_f_y,x0)
contour_with_path(f,x,y,paths,minima=np.array([[2],[2]]))

#GD vs Momentum : 데이터준비

import numpy as np
import matplotlib.pylab as plt

np.random.seed(320)
x_train = np.linspace(-1,1,51)
f = lambda x: 0.5 * x + 1.0
y_train = f(x_train) +0.4 * np.random.rand(len(x_train))

plt.plot(x_train,y_train,'o')
plt.grid()
plt.show()

np.random.seed(303)
shuffled_id = np.arange(0,len(x_train))
np.random.shuffle(shuffled_id)
x_train = x_train[shuffled_id]
y_train = y_train[shuffled_id]


def loss(w,x_set,y_set):
  N = len(x_set)
  val =0.0
  for i in range(len(x_set)):
    val += 0.5+(w[0]*x_set[i]+w[1]-y_set[i])**2
  return val/N

def loss_grad(w,x_set,y_set):
  N = len(x_set)
  val = np.zeros(len(w))
  for i in range(len(x_set)):
    er  = w[0]*x_set[i]+w[1]-y_set[i]
    val += er*np.array([x_set[i],1.0])
    return val / N


def generate_batches(batch_size, features,labels):
  assert len(features) == len(labels)
  outout_batches = []

  sample_size = len(features)
  for start_i in range(0,sample_size,batch_size):
    end_i = start_i + batch_size
    batch  = [features[start_i:end_i],labels[start_i:end_i]]
    outout_batches.append(batch)
  return outout_batches

#GD vs Momentum

# GD
batch_size =10
lr = 0.01
MaxEpochs =51

# Momentum
alpha= .9

w0 = np.array([4.0,-1.0])
path_sgd=[]
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_sgd.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    w1 = w0 - lr *grad
    w0 = w1


w0 = np.array([4.0,-1.0])
path_mm=[]
velocity = np.zeros_like(w0)
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_mm.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    velocity = alpha*velocity -lr *grad
    w1 = w0 + velocity
    w0 = w1


W0 = np.linspace(-2,5,101)
W1 = np.linspace(-2,5,101)
W0,W1 = np.meshgrid(W0,W1)
LOSSW = W0*0

for i in range(W0.shape[0]):
  for j in range(W0.shape[1]):
    LOSSW[i,j] = loss(np.array([W0[i,j],W1[i,j]]),x_train,y_train)

fig, ax = plt.subplots(figsize=(6,6))

ax.contour(W0,W1,LOSSW,cmap=plt.cm.jet, levels = np.linspace(0,max(LOSSW.flatten()),20))
paths = path_sgd
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1, color='k')

paths = path_mm
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1, color='r')

plt.legend(['GD','Moment'])
plt.show()

#GD vs Adagrad


# SGD
batch_size =10
lr = 1.5
MaxEpochs =51

# Momentum
epsilon = lr
delta = 1E-7

w0 = np.array([4.0,-1.0])
path_sgd=[]
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_sgd.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    w1 = w0 - lr *grad
    w0 = w1


w0 = np.array([4.0,-1.0])
r = np.zeros_like(w0)
path_adagd = []

velocity = np.zeros_like(w0)
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_adagd.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    r = r + grad*grad
    delw = -epsilon / (delta + np.sqrt(r)) * grad
    w1 = w0 +delw
    w0 = w1


fig, ax = plt.subplots(figsize=(6,6))

ax.contour(W0,W1,LOSSW,cmap=plt.cm.jet, levels = np.linspace(0,max(LOSSW.flatten()),20))

paths = path_sgd
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='k')

paths = path_adagd
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='r')



plt.legend(['GD','Adagrad'])
plt.show()

#Adagrad vs RMSProp


batch_size =10
MaxEpochs =51

#Adegrad
epsilon = 0.25
delta = 1E-6

# RMSProp
rho = 0.9




w0 = np.array([4.0,-1.0])
r = np.zeros_like(w0)
path_adagd = []

for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_adagd.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    r = r + grad*grad
    delw = -epsilon / (delta + np.sqrt(r)) * grad
    w1 = w0 +delw
    w0 = w1



w0 = np.array([4.0,-1.0])
r = np.zeros_like(w0)
path_rmsprop = []
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_rmsprop.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    r = rho*r + (1-rho)*grad*grad
    delw = -epsilon * grad  /np.sqrt(delta+r)


    w1 = w0+delw
    w0 = w1


fig, ax = plt.subplots(figsize=(6,6))

ax.contour(W0,W1,LOSSW,cmap=plt.cm.jet, levels = np.linspace(0,max(LOSSW.flatten()),20))



paths = path_adagd
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='k')



paths = path_rmsprop
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='r')



plt.legend(['Adagrad','RMSProp'])
plt.show()

#RMSProp vs Adam


batch_size =10
MaxEpochs =51
epsilon = 0.1
delta = 1E-6

# RMSProp
rho = 0.9

#Adam
delta_adam = 1E-8
rho1 = 0.9
rho2 = 0.999





w0 = np.array([4.0,-1.0])
r = np.zeros_like(w0)
path_rmsprop = []
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_rmsprop.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    r = rho*r + (1-rho)*grad*grad
    delw = -epsilon * grad  /np.sqrt(delta+r)
    w1 = w0+delw
    w0 = w1



w0 = np.array([4.0,-1.0])
s = np.zeros_like(w0)
r = np.zeros_like(w0)
path_adam = []
t=0
for epoch in range(MaxEpochs):

  if epoch %10 ==0:
    print(epoch,w0,loss(w0,x_train,y_train))

  for x_batch,y_batch in generate_batches(batch_size,x_train,y_train):
    path_adam.append(w0)
    grad = loss_grad(w0, x_batch,y_batch)
    s= rho1 *s +(1.-rho1) *grad
    r = rho2*r + (1-rho2)*(grad*grad)
    t+=1
    shat = s/(1.-rho1**t)
    rhat = r/(1.-rho2**t)
    delw = -epsilon * shat  /(delta_adam+ np.sqrt(rhat))
    w1 = w0+delw
    w0 = w1





fig, ax = plt.subplots(figsize=(6,6))

ax.contour(W0,W1,LOSSW,cmap=plt.cm.jet, levels = np.linspace(0,max(LOSSW.flatten()),20))



paths = path_rmsprop
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='k')



paths = path_adam
paths = np.array(np.matrix(paths).T)
ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1],
          scale_units='xy', angles='xy', scale=1,color='r')



plt.legend(['RMSProp','Ada,'])
plt.show()